{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7f79f8e0-5380-41ca-bd3a-54062ca6e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import required libraries\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5119928-e0b6-4eac-8bd8-523d204205be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Download required NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4236ca20-489b-4b8c-a8eb-e33e6271cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 CSV files:\n",
      "- apex_ad2600_dvd_player_updated.csv\n",
      "- bow_feature_importance.csv\n",
      "- canon_g3_updated.csv\n",
      "- nikon_coolpix_4300_updated.csv\n",
      "- nokia_6610_updated.csv\n",
      "- nomad_jukebox_zen_xtra_updated.csv\n",
      "- sentiment_analysis_results.csv\n",
      "Successfully read: apex_ad2600_dvd_player_updated.csv\n",
      "Shape: (740, 16), Columns: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n",
      "\n",
      "Successfully read: bow_feature_importance.csv\n",
      "Shape: (3934, 3), Columns: ['word', 'importance', 'source_file']\n",
      "\n",
      "Successfully read: canon_g3_updated.csv\n",
      "Shape: (597, 16), Columns: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n",
      "\n",
      "Successfully read: nikon_coolpix_4300_updated.csv\n",
      "Shape: (346, 16), Columns: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n",
      "\n",
      "Successfully read: nokia_6610_updated.csv\n",
      "Shape: (544, 16), Columns: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n",
      "\n",
      "Successfully read: nomad_jukebox_zen_xtra_updated.csv\n",
      "Shape: (1716, 16), Columns: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n",
      "\n",
      "Successfully read: sentiment_analysis_results.csv\n",
      "Shape: (17482, 23), Columns: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file', 'word', 'importance', 'negative', 'neutral', 'positive', 'compound', 'sentiment']\n",
      "\n",
      "\n",
      "Shape of combined dataset: (25359, 23)\n",
      "Columns in combined dataset: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file', 'word', 'importance', 'negative', 'neutral', 'positive', 'compound', 'sentiment']\n",
      "\n",
      "First few rows of combined dataset:\n",
      "   Unnamed: 0                                              title  \\\n",
      "0         0.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
      "1         1.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
      "2         2.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
      "3         3.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
      "4         4.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
      "\n",
      "                                            sentence sentiment_dict  \\\n",
      "0  repost from january 13 2004 with a better fit ...             {}   \n",
      "1  do your apex dvd player only play dvd audio wi...             {}   \n",
      "2  or do it play audio and video but scroll in bl...             {}   \n",
      "3  before you try to return the player or waste h...             {}   \n",
      "4                                         no picture             {}   \n",
      "\n",
      "   sentiment_total    [u]    [p]    [s]   [cc]   [cs]  ...  \\\n",
      "0              0.0  False  False  False  False  False  ...   \n",
      "1              0.0  False  False  False  False  False  ...   \n",
      "2              0.0  False  False  False  False  False  ...   \n",
      "3              0.0  False  False  False  False  False  ...   \n",
      "4              0.0  False  False  False  False  False  ...   \n",
      "\n",
      "                                  sentence_input_ids  \\\n",
      "0  [101, 16360, 14122, 2013, 2254, 2410, 2432, 20...   \n",
      "1  [101, 2079, 2115, 13450, 4966, 2447, 2069, 237...   \n",
      "2  [101, 2030, 2079, 2009, 2377, 5746, 1998, 2678...   \n",
      "3  [101, 2077, 2017, 3046, 2000, 2709, 1996, 2447...   \n",
      "4  [101, 2053, 3861, 102, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
      "\n",
      "                             sentence_attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "\n",
      "                          source_file word importance negative neutral  \\\n",
      "0  apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
      "1  apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
      "2  apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
      "3  apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
      "4  apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
      "\n",
      "   positive  compound  sentiment  \n",
      "0       NaN       NaN        NaN  \n",
      "1       NaN       NaN        NaN  \n",
      "2       NaN       NaN        NaN  \n",
      "3       NaN       NaN        NaN  \n",
      "4       NaN       NaN        NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# def concatenate_csv_files(directory='.'):\n",
    "#     \"\"\"\n",
    "#     Read all CSV files from the specified directory and concatenate them into a single DataFrame\n",
    "    \n",
    "#     Parameters:\n",
    "#     directory (str): Directory path where CSV files are located (defaults to current directory)\n",
    "#     \"\"\"\n",
    "#     # Get the absolute path of the directory\n",
    "#     directory_path = os.path.abspath(directory)\n",
    "    \n",
    "#     # Create the pattern for CSV files in the directory\n",
    "#     path_pattern = os.path.join(directory_path, '*.csv')\n",
    "    \n",
    "#     # Get list of all CSV files\n",
    "#     all_files = glob.glob(path_pattern)\n",
    "    \n",
    "#     # Print found files\n",
    "#     print(f\"Found {len(all_files)} CSV files:\")\n",
    "#     for file in all_files:\n",
    "#         print(f\"- {os.path.basename(file)}\")\n",
    "    \n",
    "#     # Create empty list to store individual dataframes\n",
    "#     combined_dfs = []\n",
    "    \n",
    "#     # Read each CSV file and append to list\n",
    "#     for filename in all_files:\n",
    "#         try:\n",
    "#             combined_df = pd.read_csv(filename)\n",
    "#             # Add source filename as a column (just the basename, not full path)\n",
    "#             combined_df['source_file'] = os.path.basename(filename)\n",
    "#             combined_dfs.append(combined_df)\n",
    "#             print(f\"Successfully read: {os.path.basename(filename)}\")\n",
    "#             # Print the shape and columns of each DataFrame for verification\n",
    "#             print(f\"Shape: {combined_df.shape}, Columns: {combined_df.columns.tolist()}\\n\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading {os.path.basename(filename)}: {str(e)}\")\n",
    "    \n",
    "#     # Concatenate all dataframes\n",
    "#     if combined_dfs:\n",
    "#         combined_combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "#         print(\"\\nShape of combined dataset:\", combined_combined_df.shape)\n",
    "#         print(\"Columns in combined dataset:\", combined_combined_df.columns.tolist())\n",
    "#         return combined_combined_df\n",
    "#     else:\n",
    "#         raise ValueError(\"No CSV files were successfully read\")\n",
    "\n",
    "# # Example usage:\n",
    "# try:\n",
    "#     # Call the function for the current directory\n",
    "#     combined_combined_df = concatenate_csv_files()\n",
    "    \n",
    "#     # Display the first few rows of the combined dataset\n",
    "#     print(\"\\nFirst few rows of combined dataset:\")\n",
    "#     print(combined_combined_df.head())\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1edd4d1c-46e7-475d-b15d-46415f2145b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment_dict</th>\n",
       "      <th>sentiment_total</th>\n",
       "      <th>[u]</th>\n",
       "      <th>[p]</th>\n",
       "      <th>[s]</th>\n",
       "      <th>[cc]</th>\n",
       "      <th>[cs]</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_input_ids</th>\n",
       "      <th>sentence_attention_mask</th>\n",
       "      <th>source_file</th>\n",
       "      <th>word</th>\n",
       "      <th>importance</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>troubleshoot ad-2500 and ad-2600 no picture sc...</td>\n",
       "      <td>repost from january 13 2004 with a better fit ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 16360, 14122, 2013, 2254, 2410, 2432, 20...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>apex_ad2600_dvd_player_updated.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>troubleshoot ad-2500 and ad-2600 no picture sc...</td>\n",
       "      <td>do your apex dvd player only play dvd audio wi...</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 2079, 2115, 13450, 4966, 2447, 2069, 237...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>apex_ad2600_dvd_player_updated.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>troubleshoot ad-2500 and ad-2600 no picture sc...</td>\n",
       "      <td>or do it play audio and video but scroll in bl...</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 2030, 2079, 2009, 2377, 5746, 1998, 2678...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>apex_ad2600_dvd_player_updated.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>troubleshoot ad-2500 and ad-2600 no picture sc...</td>\n",
       "      <td>before you try to return the player or waste h...</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 2077, 2017, 3046, 2000, 2709, 1996, 2447...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>apex_ad2600_dvd_player_updated.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>troubleshoot ad-2500 and ad-2600 no picture sc...</td>\n",
       "      <td>no picture</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 2053, 3861, 102, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>apex_ad2600_dvd_player_updated.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25354</th>\n",
       "      <td>1682.0</td>\n",
       "      <td>you will be very disappointed with this product</td>\n",
       "      <td>the headphone jack seem to be a major problem</td>\n",
       "      <td>{'headphone jack': -2}</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 1996, 2132, 9864, 2990, 4025, 2000, 2022...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>sentiment_analysis_results.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25355</th>\n",
       "      <td>1683.0</td>\n",
       "      <td>you will be very disappointed with this product</td>\n",
       "      <td>the one other thing i observe while i have the...</td>\n",
       "      <td>{'front cover': -2}</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 1996, 2028, 2060, 2518, 1045, 11949, 209...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>sentiment_analysis_results.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25356</th>\n",
       "      <td>1684.0</td>\n",
       "      <td>you will be very disappointed with this product</td>\n",
       "      <td>the headphone jack failure seem to occur just ...</td>\n",
       "      <td>{'headphone jack': -2}</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 1996, 2132, 9864, 2990, 4945, 4025, 2000...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>sentiment_analysis_results.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25357</th>\n",
       "      <td>1701.0</td>\n",
       "      <td>piece of junk</td>\n",
       "      <td>the other day when i be listen to a song it lo...</td>\n",
       "      <td>{'lock up': -2}</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 1996, 2060, 2154, 2043, 1045, 2022, 4952...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>sentiment_analysis_results.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25358</th>\n",
       "      <td>1711.0</td>\n",
       "      <td>nomad be wonderful but be carefull !</td>\n",
       "      <td>in that model the hard drive just die one morn...</td>\n",
       "      <td>{'hard drive': -2}</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>[101, 1999, 2008, 2944, 1996, 2524, 3298, 2074...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>sentiment_analysis_results.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25359 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              title  \\\n",
       "0             0.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
       "1             1.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
       "2             2.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
       "3             3.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
       "4             4.0  troubleshoot ad-2500 and ad-2600 no picture sc...   \n",
       "...           ...                                                ...   \n",
       "25354      1682.0    you will be very disappointed with this product   \n",
       "25355      1683.0    you will be very disappointed with this product   \n",
       "25356      1684.0    you will be very disappointed with this product   \n",
       "25357      1701.0                                      piece of junk   \n",
       "25358      1711.0               nomad be wonderful but be carefull !   \n",
       "\n",
       "                                                sentence  \\\n",
       "0      repost from january 13 2004 with a better fit ...   \n",
       "1      do your apex dvd player only play dvd audio wi...   \n",
       "2      or do it play audio and video but scroll in bl...   \n",
       "3      before you try to return the player or waste h...   \n",
       "4                                             no picture   \n",
       "...                                                  ...   \n",
       "25354      the headphone jack seem to be a major problem   \n",
       "25355  the one other thing i observe while i have the...   \n",
       "25356  the headphone jack failure seem to occur just ...   \n",
       "25357  the other day when i be listen to a song it lo...   \n",
       "25358  in that model the hard drive just die one morn...   \n",
       "\n",
       "               sentiment_dict  sentiment_total    [u]    [p]    [s]   [cc]  \\\n",
       "0                          {}              0.0  False  False  False  False   \n",
       "1                          {}              0.0  False  False  False  False   \n",
       "2                          {}              0.0  False  False  False  False   \n",
       "3                          {}              0.0  False  False  False  False   \n",
       "4                          {}              0.0  False  False  False  False   \n",
       "...                       ...              ...    ...    ...    ...    ...   \n",
       "25354  {'headphone jack': -2}             -2.0  False  False  False  False   \n",
       "25355     {'front cover': -2}             -2.0  False  False  False  False   \n",
       "25356  {'headphone jack': -2}             -2.0  False  False  False  False   \n",
       "25357         {'lock up': -2}             -2.0  False  False  False  False   \n",
       "25358      {'hard drive': -2}             -2.0  False  False  False  False   \n",
       "\n",
       "        [cs]  ...                                 sentence_input_ids  \\\n",
       "0      False  ...  [101, 16360, 14122, 2013, 2254, 2410, 2432, 20...   \n",
       "1      False  ...  [101, 2079, 2115, 13450, 4966, 2447, 2069, 237...   \n",
       "2      False  ...  [101, 2030, 2079, 2009, 2377, 5746, 1998, 2678...   \n",
       "3      False  ...  [101, 2077, 2017, 3046, 2000, 2709, 1996, 2447...   \n",
       "4      False  ...  [101, 2053, 3861, 102, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "...      ...  ...                                                ...   \n",
       "25354  False  ...  [101, 1996, 2132, 9864, 2990, 4025, 2000, 2022...   \n",
       "25355  False  ...  [101, 1996, 2028, 2060, 2518, 1045, 11949, 209...   \n",
       "25356  False  ...  [101, 1996, 2132, 9864, 2990, 4945, 4025, 2000...   \n",
       "25357  False  ...  [101, 1996, 2060, 2154, 2043, 1045, 2022, 4952...   \n",
       "25358  False  ...  [101, 1999, 2008, 2944, 1996, 2524, 3298, 2074...   \n",
       "\n",
       "                                 sentence_attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "25354  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
       "25355  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "25356  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "25357  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "25358  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                              source_file word importance negative neutral  \\\n",
       "0      apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
       "1      apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
       "2      apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
       "3      apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
       "4      apex_ad2600_dvd_player_updated.csv  NaN        NaN      NaN     NaN   \n",
       "...                                   ...  ...        ...      ...     ...   \n",
       "25354      sentiment_analysis_results.csv  NaN        NaN    0.000   1.000   \n",
       "25355      sentiment_analysis_results.csv  NaN        NaN    0.000   1.000   \n",
       "25356      sentiment_analysis_results.csv  NaN        NaN    0.000   1.000   \n",
       "25357      sentiment_analysis_results.csv  NaN        NaN    0.000   1.000   \n",
       "25358      sentiment_analysis_results.csv  NaN        NaN    0.412   0.588   \n",
       "\n",
       "       positive  compound  sentiment  \n",
       "0           NaN       NaN        NaN  \n",
       "1           NaN       NaN        NaN  \n",
       "2           NaN       NaN        NaN  \n",
       "3           NaN       NaN        NaN  \n",
       "4           NaN       NaN        NaN  \n",
       "...         ...       ...        ...  \n",
       "25354       0.0    0.0000    neutral  \n",
       "25355       0.0    0.0000    neutral  \n",
       "25356       0.0    0.0000    neutral  \n",
       "25357       0.0    0.0000    neutral  \n",
       "25358       0.0   -0.1027   negative  \n",
       "\n",
       "[25359 rows x 23 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7640b7e-5ad4-464f-86e0-33bf7626d732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8aca1b7e-ab4c-4340-94c7-e3dddc314693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded NLTK resources\n",
      "Starting sentiment analysis process...\n",
      "\n",
      "Found 5 CSV files:\n",
      "- apex_ad2600_dvd_player_updated.csv\n",
      "- canon_g3_updated.csv\n",
      "- nikon_coolpix_4300_updated.csv\n",
      "- nokia_6610_updated.csv\n",
      "- nomad_jukebox_zen_xtra_updated.csv\n",
      "\n",
      "Successfully read: apex_ad2600_dvd_player_updated.csv\n",
      "Shape after processing: (740, 16)\n",
      "\n",
      "Successfully read: canon_g3_updated.csv\n",
      "Shape after processing: (1337, 16)\n",
      "\n",
      "Successfully read: nikon_coolpix_4300_updated.csv\n",
      "Shape after processing: (1683, 16)\n",
      "\n",
      "Successfully read: nokia_6610_updated.csv\n",
      "Shape after processing: (2227, 16)\n",
      "\n",
      "Successfully read: nomad_jukebox_zen_xtra_updated.csv\n",
      "Shape after processing: (3943, 16)\n",
      "\n",
      "Shape of final deduplicated dataset: (3943, 16)\n",
      "Columns in dataset: ['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n",
      "\n",
      "No obvious text columns found. Available columns are:\n",
      "['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the name of the text column to analyze:  sentiment_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text...\n",
      "\n",
      "Performing sentiment analysis...\n",
      "\n",
      "Sentiment Analysis Results:\n",
      "\n",
      "Sentiment Distribution:\n",
      "sentiment\n",
      "neutral     3850\n",
      "positive      69\n",
      "negative      24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average Sentiment Scores:\n",
      "Negative: 0.003\n",
      "Neutral: 0.424\n",
      "Positive: 0.011\n",
      "Compound: 0.004\n",
      "\n",
      "Results saved to 'sentiment_analysis_results.csv'\n",
      "Summary statistics saved to 'sentiment_analysis_summary.txt'\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import os\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Download required NLTK data\n",
    "# try:\n",
    "#     nltk.download('punkt')\n",
    "#     nltk.download('vader_lexicon')\n",
    "#     print(\"Successfully downloaded NLTK resources\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "\n",
    "# def process_csv_and_analyze():\n",
    "#     \"\"\"\n",
    "#     Main function to process CSV files and perform sentiment analysis\n",
    "#     \"\"\"\n",
    "#     # Step 1: Get the directory path and read CSV files\n",
    "#     directory_path = os.path.abspath('.')\n",
    "#     path_pattern = os.path.join(directory_path, '*.csv')\n",
    "#     all_files = glob.glob(path_pattern)\n",
    "    \n",
    "#     print(f\"\\nFound {len(all_files)} CSV files:\")\n",
    "#     for file in all_files:\n",
    "#         print(f\"- {os.path.basename(file)}\")\n",
    "    \n",
    "#     # Step 2: Read and combine CSV files\n",
    "#     combined_df = None\n",
    "#     for filename in all_files:\n",
    "#         try:\n",
    "#             df = pd.read_csv(filename)\n",
    "#             df['source_file'] = os.path.basename(filename)\n",
    "            \n",
    "#             # If this is the first file, initialize combined_df\n",
    "#             if combined_df is None:\n",
    "#                 combined_df = df\n",
    "#             else:\n",
    "#                 # Use concat with drop_duplicates to avoid duplicate rows\n",
    "#                 combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "#                 # Drop duplicates based on all columns except 'source_file'\n",
    "#                 cols_for_dedup = [col for col in combined_df.columns if col != 'source_file']\n",
    "#                 combined_df = combined_df.drop_duplicates(subset=cols_for_dedup, keep='first')\n",
    "            \n",
    "#             print(f\"\\nSuccessfully read: {os.path.basename(filename)}\")\n",
    "#             print(f\"Shape after processing: {combined_df.shape}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading {os.path.basename(filename)}: {str(e)}\")\n",
    "    \n",
    "#     if combined_df is None:\n",
    "#         raise ValueError(\"No CSV files were successfully read\")\n",
    "    \n",
    "#     print(\"\\nShape of final deduplicated dataset:\", combined_df.shape)\n",
    "#     print(\"Columns in dataset:\", combined_df.columns.tolist())\n",
    "    \n",
    "#     # Step 3: Identify text columns\n",
    "#     possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "#     text_columns = [col for col in combined_df.columns if any(text_name in col.lower() \n",
    "#                                                             for text_name in possible_text_columns)]\n",
    "    \n",
    "#     if not text_columns:\n",
    "#         print(\"\\nNo obvious text columns found. Available columns are:\")\n",
    "#         print(combined_df.columns.tolist())\n",
    "#         text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "#     else:\n",
    "#         print(\"\\nFound potential text columns:\", text_columns)\n",
    "#         if len(text_columns) == 1:\n",
    "#             text_column = text_columns[0]\n",
    "#         else:\n",
    "#             text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    \n",
    "#     # Step 4: Preprocess text\n",
    "#     print(\"\\nPreprocessing text...\")\n",
    "#     def preprocess_text(text):\n",
    "#         if pd.isna(text):\n",
    "#             return \"\"\n",
    "#         text = str(text)\n",
    "#         tokens = word_tokenize(text.lower())\n",
    "#         return ' '.join(tokens)\n",
    "    \n",
    "#     combined_df['processed_text'] = combined_df[text_column].apply(preprocess_text)\n",
    "    \n",
    "#     # Step 5: Perform sentiment analysis\n",
    "#     print(\"\\nPerforming sentiment analysis...\")\n",
    "#     sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "#     # Apply sentiment analysis\n",
    "#     combined_df['sentiment_scores'] = combined_df['processed_text'].apply(\n",
    "#         lambda x: sid.polarity_scores(str(x)))\n",
    "    \n",
    "#     # Extract sentiment scores\n",
    "#     combined_df['negative'] = combined_df['sentiment_scores'].apply(lambda x: x['neg'])\n",
    "#     combined_df['neutral'] = combined_df['sentiment_scores'].apply(lambda x: x['neu'])\n",
    "#     combined_df['positive'] = combined_df['sentiment_scores'].apply(lambda x: x['pos'])\n",
    "#     combined_df['compound'] = combined_df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "    \n",
    "#     # Add sentiment labels\n",
    "#     combined_df['sentiment'] = combined_df['compound'].apply(\n",
    "#         lambda x: 'positive' if x >= 0.05 else 'negative' if x <= -0.05 else 'neutral')\n",
    "    \n",
    "#     # Remove the intermediate processing columns\n",
    "#     combined_df = combined_df.drop(['processed_text', 'sentiment_scores'], axis=1)\n",
    "    \n",
    "#     # Step 6: Generate analysis summary\n",
    "#     print(\"\\nSentiment Analysis Results:\")\n",
    "#     print(\"\\nSentiment Distribution:\")\n",
    "#     sentiment_dist = combined_df['sentiment'].value_counts()\n",
    "#     print(sentiment_dist)\n",
    "    \n",
    "#     print(\"\\nAverage Sentiment Scores:\")\n",
    "#     avg_scores = {\n",
    "#         'Negative': combined_df['negative'].mean(),\n",
    "#         'Neutral': combined_df['neutral'].mean(),\n",
    "#         'Positive': combined_df['positive'].mean(),\n",
    "#         'Compound': combined_df['compound'].mean()\n",
    "#     }\n",
    "#     for score_type, score in avg_scores.items():\n",
    "#         print(f\"{score_type}: {score:.3f}\")\n",
    "    \n",
    "#     # Step 7: Save results\n",
    "#     output_filename = 'nltk_sentiment_analysis_results.csv'\n",
    "#     try:\n",
    "#         combined_df.to_csv(output_filename, index=False)\n",
    "#         print(f\"\\nResults saved to '{output_filename}'\")\n",
    "        \n",
    "#         # Save summary statistics\n",
    "#         summary_filename = 'nltk_sentiment_analysis_summary.txt'\n",
    "#         with open(summary_filename, 'w') as f:\n",
    "#             f.write(\"Sentiment Analysis Summary\\n\")\n",
    "#             f.write(\"=========================\\n\\n\")\n",
    "#             f.write(\"Sentiment Distribution:\\n\")\n",
    "#             f.write(str(sentiment_dist))\n",
    "#             f.write(\"\\n\\nAverage Sentiment Scores:\\n\")\n",
    "#             for score_type, score in avg_scores.items():\n",
    "#                 f.write(f\"{score_type}: {score:.3f}\\n\")\n",
    "#         print(f\"Summary statistics saved to '{summary_filename}'\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving results: {str(e)}\")\n",
    "    \n",
    "#     return combined_df\n",
    "\n",
    "# # Execute the analysis\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         print(\"Starting sentiment analysis process...\")\n",
    "#         results_df = process_csv_and_analyze()\n",
    "#         print(\"\\nAnalysis completed successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nAn error occurred during analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ad5bd-b9a6-428d-ba77-2dd95dd61309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f6aae-d9f7-46ff-b96a-571698bd810d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5928665-b31c-42f6-8fb8-9f5693c96cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8b80392-d4c4-4ee3-86dd-977076802d33",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4af78da3-0ad7-4d02-8f85-56877f4a6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1520a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded NLTK resources\n",
      "Starting enhanced sentiment analysis process...\n",
      "\n",
      "Created output directory: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241125_135714\n",
      "\n",
      "Found 5 CSV files:\n",
      "- apex_ad2600_dvd_player_updated.csv\n",
      "- canon_g3_updated.csv\n",
      "- nikon_coolpix_4300_updated.csv\n",
      "- nokia_6610_updated.csv\n",
      "- nomad_jukebox_zen_xtra_updated.csv\n",
      "\n",
      "Processed: apex_ad2600_dvd_player_updated.csv\n",
      "Current shape: (740, 16)\n",
      "\n",
      "Processed: canon_g3_updated.csv\n",
      "Current shape: (1337, 16)\n",
      "\n",
      "Processed: nikon_coolpix_4300_updated.csv\n",
      "Current shape: (1683, 16)\n",
      "\n",
      "Processed: nokia_6610_updated.csv\n",
      "Current shape: (2227, 16)\n",
      "\n",
      "Processed: nomad_jukebox_zen_xtra_updated.csv\n",
      "Current shape: (3943, 16)\n",
      "\n",
      "No obvious text columns found. Available columns are:\n",
      "['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask', 'source_file']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the name of the text column to analyze:  sentiment_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text...\n",
      "\n",
      "Performing sentiment analysis...\n",
      "\n",
      "Results saved to: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241125_135714\\sentiment_analysis_results.csv\n",
      "Summary saved to: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241125_135714\\sentiment_analysis_summary.txt\n",
      "\n",
      "All analysis outputs saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241125_135714\n",
      "\n",
      "Analysis completed successfully!\n",
      "Results are saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\nltk_sentiment_analysis_outputs\\analysis_20241125_135714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Successfully downloaded NLTK resources\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "\n",
    "def create_output_directory():\n",
    "    \"\"\"\n",
    "    Create output directory with timestamp\n",
    "    \"\"\"\n",
    "    # Create main output directory if it doesn't exist\n",
    "    output_dir = os.path.join(os.path.abspath('.'), 'nltk_sentiment_analysis_outputs')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create timestamped subdirectory for this run\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    run_dir = os.path.join(output_dir, f'analysis_{timestamp}')\n",
    "    os.makedirs(run_dir)\n",
    "    \n",
    "    return run_dir\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text cleaning function\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text preprocessing with stopword removal and better tokenization\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean the text first\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_detailed_sentiment(compound_score, positive_score, negative_score, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Enhanced sentiment classification with more nuanced categories\n",
    "    \"\"\"\n",
    "    if compound_score >= threshold:\n",
    "        if positive_score >= 0.5:\n",
    "            return 'very positive'\n",
    "        return 'positive'\n",
    "    elif compound_score <= -threshold:\n",
    "        if negative_score >= 0.5:\n",
    "            return 'very negative'\n",
    "        return 'negative'\n",
    "    elif abs(compound_score) < 0.05:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        if positive_score > negative_score:\n",
    "            return 'slightly positive'\n",
    "        elif negative_score > positive_score:\n",
    "            return 'slightly negative'\n",
    "        return 'neutral'\n",
    "\n",
    "def process_csv_and_analyze():\n",
    "    \"\"\"\n",
    "    Enhanced main function with better sentiment analysis and organized output\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_dir = create_output_directory()\n",
    "    print(f\"\\nCreated output directory: {output_dir}\")\n",
    "    \n",
    "    # Step 1: Get the directory path and read CSV files\n",
    "    directory_path = os.path.abspath('.')\n",
    "    path_pattern = os.path.join(directory_path, '*.csv')\n",
    "    all_files = glob.glob(path_pattern)\n",
    "    \n",
    "    # Save list of processed files\n",
    "    with open(os.path.join(output_dir, 'processed_files.txt'), 'w') as f:\n",
    "        f.write(\"Processed Files:\\n\")\n",
    "        f.write(\"================\\n\\n\")\n",
    "        for file in all_files:\n",
    "            f.write(f\"- {os.path.basename(file)}\\n\")\n",
    "    \n",
    "    print(f\"\\nFound {len(all_files)} CSV files:\")\n",
    "    for file in all_files:\n",
    "        print(f\"- {os.path.basename(file)}\")\n",
    "    \n",
    "    # Step 2: Read and combine CSV files with enhanced deduplication\n",
    "    combined_df = None\n",
    "    for filename in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "            \n",
    "            # Remove any completely empty rows\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "            # Remove any complete duplicate rows\n",
    "            df = df.drop_duplicates()\n",
    "            \n",
    "            df['source_file'] = os.path.basename(filename)\n",
    "            \n",
    "            if combined_df is None:\n",
    "                combined_df = df\n",
    "            else:\n",
    "                # Concatenate and deduplicate\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "                \n",
    "                # Drop duplicates based on content columns\n",
    "                content_columns = [col for col in combined_df.columns \n",
    "                                 if col != 'source_file' and not col.startswith('sentiment_')]\n",
    "                combined_df = combined_df.drop_duplicates(subset=content_columns, keep='first')\n",
    "            \n",
    "            print(f\"\\nProcessed: {os.path.basename(filename)}\")\n",
    "            print(f\"Current shape: {combined_df.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {os.path.basename(filename)}: {str(e)}\")\n",
    "    \n",
    "    if combined_df is None:\n",
    "        raise ValueError(\"No CSV files were successfully read\")\n",
    "    \n",
    "    # Step 3: Identify text columns\n",
    "    possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "    text_columns = [col for col in combined_df.columns if any(text_name in col.lower() \n",
    "                                                            for text_name in possible_text_columns)]\n",
    "    \n",
    "    if not text_columns:\n",
    "        print(\"\\nNo obvious text columns found. Available columns are:\")\n",
    "        print(combined_df.columns.tolist())\n",
    "        text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    else:\n",
    "        print(\"\\nFound potential text columns:\", text_columns)\n",
    "        if len(text_columns) == 1:\n",
    "            text_column = text_columns[0]\n",
    "        else:\n",
    "            text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    \n",
    "    # Step 4: Enhanced text preprocessing\n",
    "    print(\"\\nPreprocessing text...\")\n",
    "    combined_df['processed_text'] = combined_df[text_column].apply(preprocess_text)\n",
    "    \n",
    "    # Remove rows where processed text is empty\n",
    "    combined_df = combined_df[combined_df['processed_text'].str.len() > 0]\n",
    "    \n",
    "    # Step 5: Enhanced sentiment analysis\n",
    "    print(\"\\nPerforming sentiment analysis...\")\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Get detailed sentiment scores\n",
    "    sentiment_scores = combined_df['processed_text'].apply(lambda x: sid.polarity_scores(str(x)))\n",
    "    combined_df['negative'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "    combined_df['neutral'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "    combined_df['positive'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "    combined_df['compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "    \n",
    "    # Apply enhanced sentiment classification\n",
    "    combined_df['sentiment'] = combined_df.apply(\n",
    "        lambda row: get_detailed_sentiment(\n",
    "            row['compound'],\n",
    "            row['positive'],\n",
    "            row['negative']\n",
    "        ), axis=1\n",
    "    )\n",
    "    \n",
    "    # Add confidence score\n",
    "    combined_df['sentiment_confidence'] = combined_df.apply(\n",
    "        lambda row: max(abs(row['compound']), max(row['positive'], row['negative'])), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    final_df = combined_df.drop(['processed_text'], axis=1)\n",
    "    \n",
    "    # Step 6: Generate enhanced analysis summary\n",
    "    sentiment_dist = final_df['sentiment'].value_counts()\n",
    "    avg_by_sentiment = final_df.groupby('sentiment')[['negative', 'neutral', 'positive', 'compound']].mean()\n",
    "    \n",
    "    # Step 7: Save all results to the output directory\n",
    "    try:\n",
    "        # Verify no duplicates exist\n",
    "        assert final_df.shape[0] == final_df.drop_duplicates().shape[0], \"Duplicates found in final dataset\"\n",
    "        \n",
    "        # Save main results\n",
    "        results_path = os.path.join(output_dir, 'sentiment_analysis_results.csv')\n",
    "        final_df.to_csv(results_path, index=False)\n",
    "        print(f\"\\nResults saved to: {results_path}\")\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary_path = os.path.join(output_dir, 'sentiment_analysis_summary.txt')\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"Enhanced Sentiment Analysis Summary\\n\")\n",
    "            f.write(\"================================\\n\\n\")\n",
    "            f.write(\"Sentiment Distribution:\\n\")\n",
    "            f.write(str(sentiment_dist))\n",
    "            f.write(\"\\n\\nAverage Scores by Sentiment Category:\\n\")\n",
    "            f.write(str(avg_by_sentiment))\n",
    "            f.write(\"\\n\\nConfidence Score Statistics:\\n\")\n",
    "            f.write(str(final_df['sentiment_confidence'].describe()))\n",
    "        print(f\"Summary saved to: {summary_path}\")\n",
    "        \n",
    "        # Save sentiment distribution plot data\n",
    "        dist_path = os.path.join(output_dir, 'sentiment_distribution.csv')\n",
    "        sentiment_dist.to_frame().to_csv(dist_path)\n",
    "        \n",
    "        # Save analysis metadata\n",
    "        meta_path = os.path.join(output_dir, 'analysis_metadata.txt')\n",
    "        with open(meta_path, 'w') as f:\n",
    "            f.write(f\"Analysis Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Files Processed: {len(all_files)}\\n\")\n",
    "            f.write(f\"Total Records Analyzed: {len(final_df)}\\n\")\n",
    "            f.write(f\"Text Column Analyzed: {text_column}\\n\")\n",
    "        \n",
    "        print(f\"\\nAll analysis outputs saved in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {str(e)}\")\n",
    "    \n",
    "    return final_df, output_dir\n",
    "\n",
    "# Execute the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Starting enhanced sentiment analysis process...\")\n",
    "        results_df, output_path = process_csv_and_analyze()\n",
    "        print(f\"\\nAnalysis completed successfully!\")\n",
    "        print(f\"Results are saved in: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a267316-e4b0-435c-baee-3414a6e94248",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8acd14f-b7af-4746-9f2f-72c075a61cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c1653-eef9-4250-993e-292b7d37c093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddffed-a97c-48c8-8bb2-185f148db5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0faad92-bbfd-48ce-aed5-ae01e1f0a3a0",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "34e8b28a-ed21-4247-86c1-e8a53b2adfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and installing required packages...\n",
      "âœ“ pandas already installed\n",
      "âœ“ numpy already installed\n",
      "Installing scikit-learn...\n",
      "âœ“ scikit-learn installed successfully\n",
      "âœ“ nltk already installed\n",
      "âœ“ wordcloud already installed\n",
      "âœ“ matplotlib already installed\n",
      "âœ“ seaborn already installed\n",
      "âœ“ Successfully downloaded NLTK resources\n",
      "\n",
      "Created output directory: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\bow_text_analysis_outputs\\bow_analysis_20241125_135845\n",
      "Read file: apex_ad2600_dvd_player_updated.csv\n",
      "Read file: canon_g3_updated.csv\n",
      "Read file: nikon_coolpix_4300_updated.csv\n",
      "Read file: nokia_6610_updated.csv\n",
      "Read file: nomad_jukebox_zen_xtra_updated.csv\n",
      "\n",
      "No obvious text columns found. Available columns are:\n",
      "['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the name of the text column to analyze:  sentiment_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Bag of Words analysis...\n",
      "Cleaning text data...\n",
      "Performing text vectorization...\n",
      "Saving analysis results...\n",
      "Generating word cloud...\n",
      "Analyzing bigrams...\n",
      "Generating summary report...\n",
      "\n",
      "Analysis completed! Results saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\bow_text_analysis_outputs\\bow_analysis_20241125_135845\n",
      "\n",
      "Files generated:\n",
      "1. word_analysis.csv - Word frequencies and TF-IDF scores\n",
      "2. bigram_analysis.csv - Common word pairs analysis\n",
      "3. wordcloud.png - Visual representation of word frequencies\n",
      "4. text_analysis_summary.txt - Detailed analysis report\n",
      "5. processed_data.csv - Processed dataset with cleaned text\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages if they're not already installed\n",
    "    \"\"\"\n",
    "    required_packages = [\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'nltk',\n",
    "        'wordcloud',\n",
    "        'matplotlib',\n",
    "        'seaborn'\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking and installing required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"âœ“ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"âœ“ {package} installed successfully\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the analysis\n",
    "    \"\"\"\n",
    "    # First install requirements\n",
    "    install_requirements()\n",
    "    \n",
    "    # Now import required packages\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from collections import Counter\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "    import seaborn as sns\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    import re\n",
    "    import glob\n",
    "    \n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"âœ“ Successfully downloaded NLTK resources\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    def create_output_directory():\n",
    "        \"\"\"\n",
    "        Create output directory with timestamp\n",
    "        \"\"\"\n",
    "        output_dir = os.path.join(os.path.abspath('.'), 'bow_text_analysis_outputs')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        run_dir = os.path.join(output_dir, f'bow_analysis_{timestamp}')\n",
    "        os.makedirs(run_dir)\n",
    "        \n",
    "        return run_dir\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Clean text for analysis\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def perform_bow_analysis(df, text_column, output_dir):\n",
    "        \"\"\"\n",
    "        Perform Bag of Words analysis\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting Bag of Words analysis...\")\n",
    "        \n",
    "        # Verify text column exists\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in dataset. Available columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Clean the text\n",
    "        print(\"Cleaning text data...\")\n",
    "        df['cleaned_text'] = df[text_column].apply(clean_text)\n",
    "        \n",
    "        # Remove empty texts\n",
    "        df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No valid text data remaining after cleaning\")\n",
    "        \n",
    "        # Initialize vectorizers\n",
    "        print(\"Performing text vectorization...\")\n",
    "        # Convert stop words to list instead of set\n",
    "        stop_words = list(stopwords.words('english'))\n",
    "        \n",
    "        # CountVectorizer for basic word frequency\n",
    "        count_vec = CountVectorizer(max_features=1000, \n",
    "                                  stop_words=stop_words,  # Now using list instead of set\n",
    "                                  min_df=2)\n",
    "        \n",
    "        # TF-IDF Vectorizer for word importance\n",
    "        tfidf_vec = TfidfVectorizer(max_features=1000,\n",
    "                                   stop_words=stop_words,  # Now using list instead of set\n",
    "                                   min_df=2)\n",
    "        \n",
    "        try:\n",
    "            # Fit and transform the text\n",
    "            bow_matrix = count_vec.fit_transform(df['cleaned_text'])\n",
    "            tfidf_matrix = tfidf_vec.fit_transform(df['cleaned_text'])\n",
    "            \n",
    "            # Get feature names\n",
    "            feature_names = count_vec.get_feature_names_out()\n",
    "            \n",
    "            # Calculate word frequencies\n",
    "            word_freq = pd.DataFrame(bow_matrix.sum(axis=0).T,\n",
    "                                   index=feature_names,\n",
    "                                   columns=['frequency']).sort_values('frequency', ascending=False)\n",
    "            \n",
    "            # Calculate TF-IDF scores\n",
    "            tfidf_scores = pd.DataFrame(tfidf_matrix.mean(axis=0).T,\n",
    "                                      index=feature_names,\n",
    "                                      columns=['tfidf_score']).sort_values('tfidf_score', ascending=False)\n",
    "            \n",
    "            # Combine frequencies and TF-IDF scores\n",
    "            word_analysis = pd.merge(word_freq, tfidf_scores,\n",
    "                                   left_index=True, right_index=True,\n",
    "                                   how='outer').fillna(0)\n",
    "            \n",
    "            # Save results\n",
    "            print(\"Saving analysis results...\")\n",
    "            word_analysis.to_csv(os.path.join(output_dir, 'word_analysis.csv'))\n",
    "            \n",
    "            # Generate and save word clouds\n",
    "            print(\"Generating word cloud...\")\n",
    "            plt.figure(figsize=(20,10))\n",
    "            try:\n",
    "                wordcloud = WordCloud(width=1600, height=800,\n",
    "                                    background_color='white',\n",
    "                                    max_words=100).generate_from_frequencies(\n",
    "                                        dict(zip(word_freq.index, word_freq['frequency']))\n",
    "                                    )\n",
    "                \n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                plt.title('Word Cloud of Most Frequent Terms')\n",
    "                plt.savefig(os.path.join(output_dir, 'wordcloud.png'), bbox_inches='tight', dpi=300)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not generate word cloud: {str(e)}\")\n",
    "            finally:\n",
    "                plt.close()\n",
    "            \n",
    "            # Calculate bigrams\n",
    "            print(\"Analyzing bigrams...\")\n",
    "            bigram_vectorizer = CountVectorizer(ngram_range=(2,2),\n",
    "                                              max_features=100,\n",
    "                                              stop_words=stop_words)  # Now using list instead of set\n",
    "            bigram_matrix = bigram_vectorizer.fit_transform(df['cleaned_text'])\n",
    "            bigram_freq = pd.DataFrame(bigram_matrix.sum(axis=0).T,\n",
    "                                      index=bigram_vectorizer.get_feature_names_out(),\n",
    "                                      columns=['frequency']).sort_values('frequency', ascending=False)\n",
    "            \n",
    "            bigram_freq.to_csv(os.path.join(output_dir, 'bigram_analysis.csv'))\n",
    "            \n",
    "            # Generate summary report\n",
    "            print(\"Generating summary report...\")\n",
    "            with open(os.path.join(output_dir, 'text_analysis_summary.txt'), 'w') as f:\n",
    "                f.write(\"Text Analysis Summary\\n\")\n",
    "                f.write(\"===================\\n\\n\")\n",
    "                \n",
    "                f.write(\"Document Statistics:\\n\")\n",
    "                f.write(f\"Total documents analyzed: {len(df)}\\n\")\n",
    "                f.write(f\"Average document length: {df['cleaned_text'].str.len().mean():.1f} characters\\n\")\n",
    "                f.write(f\"Unique words analyzed: {len(feature_names)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Top 20 Most Frequent Words:\\n\")\n",
    "                f.write(str(word_freq.head(20)))\n",
    "                f.write(\"\\n\\nTop 20 Most Important Words (TF-IDF):\\n\")\n",
    "                f.write(str(tfidf_scores.head(20)))\n",
    "                f.write(\"\\n\\nTop 20 Most Common Bigrams:\\n\")\n",
    "                f.write(str(bigram_freq.head(20)))\n",
    "            \n",
    "            # Save processed dataset\n",
    "            df.to_csv(os.path.join(output_dir, 'processed_data.csv'), index=False)\n",
    "            \n",
    "            return word_analysis, bigram_freq, df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during text analysis: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = create_output_directory()\n",
    "    print(f\"\\nCreated output directory: {output_dir}\")\n",
    "    \n",
    "    # Read all CSV files in directory\n",
    "    csv_files = glob.glob('*.csv')\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in current directory!\")\n",
    "        return\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            print(f\"Read file: {file}\")\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"No valid CSV files could be read!\")\n",
    "        return\n",
    "        \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Identify text column\n",
    "    possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "    text_columns = [col for col in combined_df.columns \n",
    "                   if any(text_name in col.lower() for text_name in possible_text_columns)]\n",
    "    \n",
    "    if not text_columns:\n",
    "        print(\"\\nNo obvious text columns found. Available columns are:\")\n",
    "        print(combined_df.columns.tolist())\n",
    "        text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    else:\n",
    "        print(\"\\nFound potential text columns:\", text_columns)\n",
    "        if len(text_columns) == 1:\n",
    "            text_column = text_columns[0]\n",
    "        else:\n",
    "            text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    \n",
    "    try:\n",
    "        # Perform analysis\n",
    "        word_analysis, bigram_freq, processed_df = perform_bow_analysis(combined_df, text_column, output_dir)\n",
    "        \n",
    "        print(f\"\\nAnalysis completed! Results saved in: {output_dir}\")\n",
    "        print(\"\\nFiles generated:\")\n",
    "        print(\"1. word_analysis.csv - Word frequencies and TF-IDF scores\")\n",
    "        print(\"2. bigram_analysis.csv - Common word pairs analysis\")\n",
    "        print(\"3. wordcloud.png - Visual representation of word frequencies\")\n",
    "        print(\"4. text_analysis_summary.txt - Detailed analysis report\")\n",
    "        print(\"5. processed_data.csv - Processed dataset with cleaned text\")\n",
    "        \n",
    "        return word_analysis, bigram_freq, processed_df, output_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during analysis: {str(e)}\")\n",
    "        return None, None, None, output_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        if results[0] is not None:\n",
    "            print(f\"\\nAnalysis completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nAnalysis completed with errors. Please check the output directory for partial results.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ccb70c-74b7-4e47-ad11-527d4876fdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a2e69-b730-4be2-a6c8-ad3e9bd45f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a408e-ad01-4a2e-945a-0a504b5b1dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bf6a14d-43bb-4fcf-8457-2bd2ce081b04",
   "metadata": {},
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "70f4cb99-da23-4819-88e1-ae90c446585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and installing required packages...\n",
      "âœ“ pandas already installed\n",
      "âœ“ numpy already installed\n",
      "Installing scikit-learn...\n",
      "âœ“ scikit-learn installed successfully\n",
      "âœ“ nltk already installed\n",
      "âœ“ matplotlib already installed\n",
      "âœ“ seaborn already installed\n",
      "âœ“ Successfully downloaded NLTK resources\n",
      "\n",
      "Created output directory: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\tfidf_analysis_outputs\\tfidf_analysis_20241125_140148\n",
      "Read file: apex_ad2600_dvd_player_updated.csv\n",
      "Read file: canon_g3_updated.csv\n",
      "Read file: nikon_coolpix_4300_updated.csv\n",
      "Read file: nokia_6610_updated.csv\n",
      "Read file: nomad_jukebox_zen_xtra_updated.csv\n",
      "\n",
      "No obvious text columns found. Available columns are:\n",
      "['Unnamed: 0', 'title', 'sentence', 'sentiment_dict', 'sentiment_total', '[u]', '[p]', '[s]', '[cc]', '[cs]', 'annotations', 'title_input_ids', 'title_attention_mask', 'sentence_input_ids', 'sentence_attention_mask']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the name of the text column to analyze:  sentiment_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting TF-IDF analysis...\n",
      "Cleaning text data...\n",
      "Performing TF-IDF vectorization...\n",
      "Saving analysis results...\n",
      "Generating visualizations...\n",
      "Calculating document similarity...\n",
      "Generating summary report...\n",
      "\n",
      "Analysis completed! Results saved in: C:\\Users\\1520a\\--- MSU MSDS ---\\CSE 482\\Project\\tfidf_analysis_outputs\\tfidf_analysis_20241125_140148\n",
      "\n",
      "Files generated:\n",
      "1. average_tfidf_scores.csv - Average TF-IDF scores for each term\n",
      "2. document_term_matrix.csv - Full document-term matrix\n",
      "3. top_terms_tfidf.png - Bar plot of top terms by TF-IDF score\n",
      "4. tfidf_distribution.png - Distribution of TF-IDF scores\n",
      "5. document_similarity.png - Heatmap of document similarity\n",
      "6. tfidf_analysis_summary.txt - Detailed analysis report\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages if they're not already installed\n",
    "    \"\"\"\n",
    "    required_packages = [\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'nltk',\n",
    "        'matplotlib',\n",
    "        'seaborn'\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking and installing required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"âœ“ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"âœ“ {package} installed successfully\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the TF-IDF analysis\n",
    "    \"\"\"\n",
    "    # First install requirements\n",
    "    install_requirements()\n",
    "    \n",
    "    # Now import required packages\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from nltk.corpus import stopwords\n",
    "    import nltk\n",
    "    import re\n",
    "    import glob\n",
    "    \n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        print(\"âœ“ Successfully downloaded NLTK resources\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK resources: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    def create_output_directory():\n",
    "        \"\"\"Create output directory with timestamp\"\"\"\n",
    "        output_dir = os.path.join(os.path.abspath('.'), 'tfidf_analysis_outputs')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        run_dir = os.path.join(output_dir, f'tfidf_analysis_{timestamp}')\n",
    "        os.makedirs(run_dir)\n",
    "        \n",
    "        return run_dir\n",
    "\n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean text for analysis\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def perform_tfidf_analysis(df, text_column, output_dir):\n",
    "        \"\"\"Perform TF-IDF analysis\"\"\"\n",
    "        print(\"\\nStarting TF-IDF analysis...\")\n",
    "        \n",
    "        # Verify text column exists\n",
    "        if text_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{text_column}' not found in dataset. Available columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Clean the text\n",
    "        print(\"Cleaning text data...\")\n",
    "        df['cleaned_text'] = df[text_column].apply(clean_text)\n",
    "        \n",
    "        # Remove empty texts\n",
    "        df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No valid text data remaining after cleaning\")\n",
    "        \n",
    "        # Initialize vectorizer\n",
    "        print(\"Performing TF-IDF vectorization...\")\n",
    "        stop_words = list(stopwords.words('english'))\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words=stop_words,\n",
    "            min_df=2,         # Ignore terms that appear in less than 2 documents\n",
    "            max_df=0.95,      # Ignore terms that appear in more than 95% of documents\n",
    "            ngram_range=(1,2) # Include both unigrams and bigrams\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Fit and transform the text\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "            feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Convert to dense array for analysis\n",
    "            dense_tfidf = tfidf_matrix.todense()\n",
    "            \n",
    "            # Calculate document-term importance\n",
    "            doc_term_matrix = pd.DataFrame(\n",
    "                dense_tfidf,\n",
    "                columns=feature_names\n",
    "            )\n",
    "            \n",
    "            # Calculate average TF-IDF scores across all documents\n",
    "            avg_tfidf = pd.DataFrame({\n",
    "                'term': feature_names,\n",
    "                'avg_tfidf': doc_term_matrix.mean().values,\n",
    "                'docs_present': (doc_term_matrix > 0).sum().values\n",
    "            }).sort_values('avg_tfidf', ascending=False)\n",
    "            \n",
    "            # Save results\n",
    "            print(\"Saving analysis results...\")\n",
    "            \n",
    "            # Save average TF-IDF scores\n",
    "            avg_tfidf.to_csv(os.path.join(output_dir, 'average_tfidf_scores.csv'), index=False)\n",
    "            \n",
    "            # Save document-term matrix\n",
    "            doc_term_matrix.to_csv(os.path.join(output_dir, 'document_term_matrix.csv'))\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"Generating visualizations...\")\n",
    "            \n",
    "            # Plot top terms by average TF-IDF score\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            sns.barplot(data=avg_tfidf.head(20), x='avg_tfidf', y='term')\n",
    "            plt.title('Top 20 Terms by Average TF-IDF Score')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'top_terms_tfidf.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot term frequency distribution\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            sns.histplot(data=avg_tfidf, x='avg_tfidf', bins=50)\n",
    "            plt.title('Distribution of TF-IDF Scores')\n",
    "            plt.xlabel('TF-IDF Score')\n",
    "            plt.ylabel('Count')\n",
    "            plt.savefig(os.path.join(output_dir, 'tfidf_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Generate document similarity matrix\n",
    "            print(\"Calculating document similarity...\")\n",
    "            similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "            \n",
    "            # Plot document similarity heatmap\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(similarity_matrix[:50, :50], cmap='YlOrRd')  # Limited to first 50 documents for visibility\n",
    "            plt.title('Document Similarity Matrix (First 50 Documents)')\n",
    "            plt.savefig(os.path.join(output_dir, 'document_similarity.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Generate summary report\n",
    "            print(\"Generating summary report...\")\n",
    "            with open(os.path.join(output_dir, 'tfidf_analysis_summary.txt'), 'w') as f:\n",
    "                f.write(\"TF-IDF Analysis Summary\\n\")\n",
    "                f.write(\"=====================\\n\\n\")\n",
    "                \n",
    "                f.write(\"Document Statistics:\\n\")\n",
    "                f.write(f\"Total documents analyzed: {len(df)}\\n\")\n",
    "                f.write(f\"Average document length: {df['cleaned_text'].str.len().mean():.1f} characters\\n\")\n",
    "                f.write(f\"Unique terms analyzed: {len(feature_names)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Top 20 Most Important Terms (by average TF-IDF):\\n\")\n",
    "                f.write(avg_tfidf.head(20).to_string())\n",
    "                f.write(\"\\n\\nTF-IDF Score Statistics:\\n\")\n",
    "                f.write(f\"Mean TF-IDF score: {avg_tfidf['avg_tfidf'].mean():.4f}\\n\")\n",
    "                f.write(f\"Median TF-IDF score: {avg_tfidf['avg_tfidf'].median():.4f}\\n\")\n",
    "                f.write(f\"Max TF-IDF score: {avg_tfidf['avg_tfidf'].max():.4f}\\n\")\n",
    "                \n",
    "                # Find most unique terms (high TF-IDF, low document frequency)\n",
    "                unique_terms = avg_tfidf[avg_tfidf['docs_present'] <= len(df) * 0.1].head(20)\n",
    "                f.write(\"\\n\\nMost Unique Terms (high TF-IDF, present in <10% of documents):\\n\")\n",
    "                f.write(unique_terms.to_string())\n",
    "            \n",
    "            return doc_term_matrix, avg_tfidf, similarity_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during TF-IDF analysis: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = create_output_directory()\n",
    "    print(f\"\\nCreated output directory: {output_dir}\")\n",
    "    \n",
    "    # Read all CSV files in directory\n",
    "    csv_files = glob.glob('*.csv')\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in current directory!\")\n",
    "        return\n",
    "    \n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            print(f\"Read file: {file}\")\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        print(\"No valid CSV files could be read!\")\n",
    "        return\n",
    "        \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Identify text column\n",
    "    possible_text_columns = ['text', 'description', 'comment', 'review', 'content', 'message']\n",
    "    text_columns = [col for col in combined_df.columns \n",
    "                   if any(text_name in col.lower() for text_name in possible_text_columns)]\n",
    "    \n",
    "    if not text_columns:\n",
    "        print(\"\\nNo obvious text columns found. Available columns are:\")\n",
    "        print(combined_df.columns.tolist())\n",
    "        text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    else:\n",
    "        print(\"\\nFound potential text columns:\", text_columns)\n",
    "        if len(text_columns) == 1:\n",
    "            text_column = text_columns[0]\n",
    "        else:\n",
    "            text_column = input(\"\\nPlease enter the name of the text column to analyze: \")\n",
    "    \n",
    "    try:\n",
    "        # Perform analysis\n",
    "        doc_term_matrix, avg_tfidf, similarity_matrix = perform_tfidf_analysis(combined_df, text_column, output_dir)\n",
    "        \n",
    "        print(f\"\\nAnalysis completed! Results saved in: {output_dir}\")\n",
    "        print(\"\\nFiles generated:\")\n",
    "        print(\"1. average_tfidf_scores.csv - Average TF-IDF scores for each term\")\n",
    "        print(\"2. document_term_matrix.csv - Full document-term matrix\")\n",
    "        print(\"3. top_terms_tfidf.png - Bar plot of top terms by TF-IDF score\")\n",
    "        print(\"4. tfidf_distribution.png - Distribution of TF-IDF scores\")\n",
    "        print(\"5. document_similarity.png - Heatmap of document similarity\")\n",
    "        print(\"6. tfidf_analysis_summary.txt - Detailed analysis report\")\n",
    "        \n",
    "        return doc_term_matrix, avg_tfidf, similarity_matrix, output_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during analysis: {str(e)}\")\n",
    "        return None, None, None, output_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        if results[0] is not None:\n",
    "            print(f\"\\nAnalysis completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nAnalysis completed with errors. Please check the output directory for partial results.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db023347-42e4-4144-8bed-96645a57458e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
